#!/usr/bin/env python3
"""
Standalone PDF Parser for Testing and Debugging

This script replicates the DocumentParser functionality but runs independently
for testing and debugging purposes. It includes enhanced debugging for page
number extraction and stores results locally.

Usage:
    python standalone_parser.py /path/to/your/document.pdf
"""

import argparse
import base64
import json
import logging
import os
import re
import sys
import textwrap
import time
from collections import Counter
from collections.abc import Generator, Iterable, Sequence
from dataclasses import dataclass
from datetime import datetime
from io import BytesIO
from pathlib import Path
from typing import Any, Callable, Dict, Optional
from uuid import uuid4

import tiktoken
from docling.datamodel.base_models import ConfidenceReport, InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling_core.transforms.chunker.hybrid_chunker import HybridChunker
from docling_core.transforms.chunker.tokenizer.openai import OpenAITokenizer
from docling_core.transforms.serializer.markdown import MarkdownDocSerializer
from docling_core.types.doc.document import DoclingDocument
from docling_core.types.io import DocumentStream

try:  # Optional improved extraction / rendering
    import fitz  # type: ignore[import]
except ImportError:  # pragma: no cover
    fitz = None

# Use standalone configuration - no backend dependencies
OPENAI_MODEL = "gpt-4o"  # Default model for testing

# Try to import backend settings if available, but don't fail if not
try:
    # Only try if we're in the right environment
    backend_src = Path(__file__).parent.parent / "src"
    if backend_src.exists():
        sys.path.append(str(backend_src))
        try:
            from em_backend.core.config import settings
            OPENAI_MODEL = settings.openai_model_name
            print(f"‚úÖ SUCCESS: Using backend model: {OPENAI_MODEL}")
        except Exception:
            print(f"‚ùå FAILED: Backend config import failed, using fallback model: {OPENAI_MODEL}")
    else:
        print(f"‚ùå FAILED: Backend source not found, using fallback model: {OPENAI_MODEL}")
except Exception:
    print(f"‚ùå FAILED: Exception during config import, using fallback model: {OPENAI_MODEL}")

# Configure extensive logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(Path(__file__).parent / "debug.log", mode='w')  # Overwrite each run
    ]
)

logger = logging.getLogger("standalone_parser")


@dataclass
class PageExtraction:
    page_number: int
    text: str
    method: str
    needs_vision: bool = False


@dataclass
class OpenAIVisionConfig:
    enabled: bool
    model: str
    max_retries: int
    system_prompt: str
    user_prompt_template: str
    max_output_tokens: Optional[int]
    retry_backoff_seconds: float


def build_openai_vision_config() -> OpenAIVisionConfig:
    enabled = os.getenv("VISION_ENABLED", "false").lower() in {"1", "true", "yes", "on"}
    model = os.getenv("VISION_MODEL", "gpt-4o-mini")
    max_retries = int(os.getenv("VISION_MAX_RETRIES", "2"))
    max_output_tokens_env = os.getenv("VISION_MAX_OUTPUT_TOKENS")
    max_output_tokens = int(max_output_tokens_env) if max_output_tokens_env else None
    retry_backoff_seconds = float(os.getenv("VISION_RETRY_BACKOFF", "2.0"))

    default_system_prompt = (
        "You are an assistant that transcribes textual content from scanned document pages. "
        "Return clean plain text in the original language, preserving headings and structure. "
        "If you cannot read the page, respond exactly with 'UNREADABLE'."
    )
    system_prompt = os.getenv("VISION_SYSTEM_PROMPT", default_system_prompt)

    default_user_prompt = (
        "This is page {page_number} of the document '{doc_name}'. Extract all readable text "
        "and present it as plain text. Preserve headings, bullet points, and paragraph breaks. "
        "If you cannot read the page, respond with 'UNREADABLE'."
    )
    user_prompt_template = os.getenv("VISION_USER_PROMPT_TEMPLATE", default_user_prompt)

    return OpenAIVisionConfig(
        enabled=enabled,
        model=model,
        max_retries=max_retries,
        system_prompt=system_prompt,
        user_prompt_template=user_prompt_template,
        max_output_tokens=max_output_tokens,
        retry_backoff_seconds=retry_backoff_seconds,
    )


def text_requires_ocr(text: str) -> bool:
    stripped = text.strip()
    if not stripped:
        return True
    alpha_chars = sum(ch.isalpha() for ch in stripped)
    if alpha_chars >= 30:
        return False
    lower = stripped.lower()
    if "/gid" in lower or "gid" in lower:
        return True
    gid_hits = lower.count("gid")
    word_count = max(1, len(stripped.split()))
    gid_ratio = gid_hits / word_count
    if gid_ratio > 0.2:
        return True
    alpha_ratio = alpha_chars / max(len(stripped), 1)
    return alpha_ratio < 0.05
def render_pages_to_images(pdf_path: Path, page_numbers: Sequence[int], dpi: int = 300) -> Dict[int, bytes]:
    if fitz is None:
        logger.error("PyMuPDF is required to render pages for vision fallback.")
        return {}

    images: Dict[int, bytes] = {}
    try:
        doc = fitz.open(str(pdf_path))  # type: ignore[operator]
    except Exception as exc:
        logger.exception("Unable to open PDF with PyMuPDF for rendering: %s", exc)
        return {}

    try:
        scale = dpi / 72.0
        matrix = fitz.Matrix(scale, scale)
        for page_number in page_numbers:
            try:
                page = doc.load_page(page_number - 1)
                pix = page.get_pixmap(matrix=matrix)
                images[page_number] = pix.tobytes("png")
            except Exception as exc:  # pragma: no cover
                logger.exception("Failed to render page %s for vision fallback: %s", page_number, exc)
    finally:
        doc.close()

    return images


def recover_pages_with_openai(
    pdf_path: Path,
    page_numbers: Sequence[int],
    vision_config: OpenAIVisionConfig,
) -> Dict[int, str]:
    if not vision_config.enabled:
        logger.info("Vision fallback disabled via configuration.")
        return {}

    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        logger.warning("OPENAI_API_KEY not set; skipping vision fallback.")
        return {}

    try:
        from openai import OpenAI  # type: ignore[import]
    except ImportError:  # pragma: no cover
        logger.error("OpenAI python client not installed. Cannot use vision fallback.")
        return {}

    images = render_pages_to_images(pdf_path, page_numbers)
    if not images:
        logger.warning("No images rendered for vision fallback; skipping.")
        return {}

    client = OpenAI()
    recovered: Dict[int, str] = {}

    for page_number in page_numbers:
        image_bytes = images.get(page_number)
        if not image_bytes:
            continue

        prompt = vision_config.user_prompt_template.format(
            doc_name=pdf_path.name,
            page_number=page_number,
        )

        payload = base64.b64encode(image_bytes).decode("utf-8")
        attempt = 0
        while attempt <= vision_config.max_retries:
            try:
                messages = [
                    {
                        "role": "system",
                        "content": vision_config.system_prompt,
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {"url": f"data:image/png;base64,{payload}", "detail": "auto"},
                            },
                        ],
                    },
                ]

                response = client.chat.completions.create(
                    model=vision_config.model,
                    messages=messages,
                    max_tokens=vision_config.max_output_tokens,
                )
            except Exception as exc:  # pragma: no cover
                attempt += 1
                logger.exception(
                    "Vision API call failed for page %s (attempt %s/%s): %s",
                    page_number,
                    attempt,
                    vision_config.max_retries,
                    exc,
                )
                if attempt > vision_config.max_retries:
                    logger.error("Max retries exceeded for page %s vision fallback", page_number)
                    break
                time.sleep(vision_config.retry_backoff_seconds * attempt)
                continue

            text = ""
            try:
                for choice in getattr(response, "choices", []) or []:
                    message = getattr(choice, "message", None)
                    content = getattr(message, "content", None)
                    if content:
                        text = content.strip()
                        if text:
                            break
            except Exception:  # pragma: no cover
                text = ""

            text = (text or "").strip()
            if not text:
                logger.warning("Vision model returned empty text for page %s", page_number)
            elif text.upper() == "UNREADABLE":
                logger.warning("Vision model reported page %s as unreadable", page_number)
            else:
                recovered[page_number] = text
                logger.info(
                    "Vision model recovered %s characters for page %s",
                    len(text),
                    page_number,
                )
            break

    return recovered

# Also configure docling loggers to be more verbose
docling_logger = logging.getLogger("docling")
docling_logger.setLevel(logging.DEBUG)

# Create a separate handler for very detailed logs
detailed_handler = logging.FileHandler(Path(__file__).parent / "detailed_debug.log", mode='w')
detailed_handler.setLevel(logging.DEBUG)
detailed_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s')
detailed_handler.setFormatter(detailed_formatter)
logger.addHandler(detailed_handler)


class StandaloneDocumentParser:
    """
    Standalone Document Parser for testing and debugging.
    
    This class replicates the DocumentParser functionality but with enhanced
    debugging capabilities, especially for page number extraction.
    """

    MAX_CHUNK_TOKENS = 2000
    MIN_CHUNK_TOKENS = 1500  # Ensure chunks have at least 1500 tokens
    CHUNK_OVERLAP_TOKENS = 150
    _PLACEHOLDER_MARKERS = {"<!-- missing-text -->"}
    _GID_TOKEN = "/gid"
    MAX_DEBUG_ITEMS = 5

    def __init__(self, debug_mode: bool = True) -> None:
        logger.info("üöÄ INITIALIZING StandaloneDocumentParser...")
        logger.debug(f"Debug mode: {debug_mode}")
        
        self.debug_mode = debug_mode
        
        # Setup Document converter using docling models
        logger.info("üìÑ Setting up DocumentConverter...")
        
        # Try multiple model cache locations in order of preference
        potential_caches = [
            Path(__file__).parent.parent / ".cache" / "docling" / "models",  # Project cache
            Path.home() / ".cache" / "docling" / "models",  # Home cache (where models were downloaded)
        ]
        
        cache_path = None
        for path in potential_caches:
            if path.exists() and (path / "model.safetensors").exists():
                cache_path = path
                logger.info(f"‚úÖ Found models at: {cache_path}")
                break
        
        if cache_path is None:
            # Use home cache anyway (models might be there even if we can't check)
            cache_path = Path.home() / ".cache" / "docling" / "models"
            logger.warning(f"‚ö†Ô∏è Model validation failed, using default: {cache_path}")
        
        logger.debug(f"üìÅ Model cache path: {cache_path}")
        logger.debug(f"   Cache exists: {cache_path.exists()}")
        
        # Check what models are available
        if cache_path.exists():
            model_files = list(cache_path.rglob("*.safetensors"))
            logger.info(f"‚úÖ Found {len(model_files)} model files:")
            for model in model_files[:5]:  # Show first 5
                logger.debug(f"   - {model.relative_to(cache_path)}")
            if len(model_files) > 5:
                logger.debug(f"   ... and {len(model_files) - 5} more")
        
        self._model_cache_path = cache_path
        try:
            # Use standard pipeline without docling OCR; rely on external vision fallback instead
            logger.info("üîß Initializing DocumentConverter with standard pipeline (OCR disabled)...")

            pipeline_options = PdfPipelineOptions(
                artifacts_path=cache_path,
                do_ocr=False,
                do_table_structure=True,
            )

            self.doc_converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=pipeline_options
                    )
                }
            )
            logger.info("‚úÖ SUCCESS: DocumentConverter initialized with OCR disabled")
            logger.info(f"   Using models from: {cache_path}")
            logger.info("   Features: Layout detection ‚úÖ, Table structure ‚úÖ, OCR ‚ùå (handled by OpenAI vision fallback)")

        except FileNotFoundError as e:
            logger.warning("‚ö†Ô∏è OCR models missing (%s). Falling back to text-only pipeline.", e)
            logger.warning("   Run `docling-tools models download` to enable OCR.")
            pipeline_options = PdfPipelineOptions(
                artifacts_path=cache_path,
                do_ocr=False,
                do_table_structure=True,
            )
            self.doc_converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(
                        pipeline_options=pipeline_options
                    )
                }
            )
            logger.info("‚úÖ SUCCESS: DocumentConverter initialized without OCR fallback")
        except Exception as e:
            logger.error(f"‚ùå FAILED: DocumentConverter initialization error: {e}")
            logger.error(f"   Exception type: {type(e)}")
            logger.error(f"   Model cache: {cache_path}")
            logger.error("   ")
            logger.error("   üí° To download models, run:")
            logger.error("      docling-tools models download")
            raise

        # Setup Document chunker
        logger.info("üß© Setting up chunking components...")
        logger.debug(f"Using OpenAI model: {OPENAI_MODEL}")
        
        try:
            self._encoding = tiktoken.encoding_for_model(OPENAI_MODEL)
            logger.info(f"‚úÖ SUCCESS: Tiktoken encoding initialized for model: {OPENAI_MODEL}")
        except Exception as e:
            logger.error(f"‚ùå FAILED: Tiktoken encoding initialization error: {e}")
            raise
        
        try:
            self.tokenizer = OpenAITokenizer(
                tokenizer=self._encoding,
                max_tokens=self.MAX_CHUNK_TOKENS,
            )
            logger.info(f"‚úÖ SUCCESS: OpenAI tokenizer initialized with max_tokens: {self.MAX_CHUNK_TOKENS}")
        except Exception as e:
            logger.error(f"‚ùå FAILED: OpenAI tokenizer initialization error: {e}")
            raise
        
        try:
            self.chunker = HybridChunker(
                tokenizer=self.tokenizer,
                chunk_size=self.MAX_CHUNK_TOKENS,
                chunk_overlap=self.CHUNK_OVERLAP_TOKENS,
            )
            logger.info("‚úÖ SUCCESS: HybridChunker initialized successfully")
        except Exception as e:
            logger.error(f"‚ùå FAILED: HybridChunker initialization error: {e}")
            raise
        
        logger.info(f"üéØ Parser configuration summary:")
        logger.info(f"   ‚úÖ Model: {OPENAI_MODEL}")
        logger.info(f"   ‚úÖ Min chunk tokens: {self.MIN_CHUNK_TOKENS}")
        logger.info(f"   ‚úÖ Max chunk tokens: {self.MAX_CHUNK_TOKENS}")
        logger.info(f"   ‚úÖ Chunk overlap tokens: {self.CHUNK_OVERLAP_TOKENS}")
        logger.info(f"   ‚úÖ Debug mode: {debug_mode}")
        logger.info("üéâ SUCCESS: StandaloneDocumentParser initialization COMPLETE!")
        self._current_pdf_path: Path | None = None
        self._fitz_doc = None
        self._model_cache_path = cache_path
        self.vision_config = build_openai_vision_config()

    def parse_document_from_file(self, file_path: Path) -> tuple[DoclingDocument, ConfidenceReport]:
        """Parse a document from a file path."""
        logger.info(f"üìÅ Starting document parsing from file: {file_path}")
        logger.debug(f"File path type: {type(file_path)}")
        logger.debug(f"File path absolute: {file_path.absolute()}")
        
        # Validate file existence
        if not file_path.exists():
            logger.error(f"‚ùå File does not exist: {file_path}")
            raise ValueError(f"File does not exist: {file_path}")
        logger.debug("‚úÖ File exists")
        
        if not file_path.is_file():
            logger.error(f"‚ùå Path is not a file: {file_path}")
            raise ValueError(f"Path is not a file: {file_path}")
        logger.debug("‚úÖ Path is a valid file")
        
        # Get file info
        file_stat = file_path.stat()
        file_size = file_stat.st_size
        logger.info(f"üìä File info:")
        logger.info(f"   - Name: {file_path.name}")
        logger.info(f"   - Size: {file_size:,} bytes ({file_size / 1024:.1f} KB)")
        logger.info(f"   - Extension: {file_path.suffix}")
        logger.debug(f"   - Modified: {datetime.fromtimestamp(file_stat.st_mtime)}")
        
        # Read file content
        logger.info("üìñ Reading file content...")
        try:
            file_content = file_path.read_bytes()
            logger.debug(f"‚úÖ Successfully read {len(file_content):,} bytes")
        except Exception as e:
            logger.error(f"‚ùå Failed to read file: {e}")
            raise
        
        # Remember path for fallbacks
        self._current_pdf_path = file_path
        self._fitz_doc = self._load_fitz_document(file_path)

        # Create BytesIO stream
        logger.debug("üîÑ Creating BytesIO stream...")
        file_stream = BytesIO(file_content)
        logger.debug(f"‚úÖ BytesIO stream created, size: {len(file_stream.getvalue()):,} bytes")
        
        # Create DocumentStream
        logger.debug("üìÑ Creating DocumentStream...")
        doc_stream = DocumentStream(name=file_path.name, stream=file_stream)
        logger.debug(f"‚úÖ DocumentStream created with name: {doc_stream.name}")
        
        # Parse document
        logger.info("üîß Starting document conversion with docling...")
        try:
            result = self.doc_converter.convert(doc_stream)
            logger.info("‚úÖ Document conversion completed successfully!")
        except FileNotFoundError as e:
            logger.warning("‚ö†Ô∏è OCR resources missing during conversion (%s). Retrying without OCR.", e)
            self._initialize_converter_without_ocr()
            result = self.doc_converter.convert(doc_stream)
            logger.info("‚úÖ Document conversion completed after retry without OCR.")
        except Exception as e:
            logger.error(f"‚ùå Document conversion failed: {e}")
            logger.error(f"Exception type: {type(e)}")
            raise
        
        # Analyze results
        logger.info("üìä Analyzing conversion results...")
        logger.info(f"   - Confidence mean grade: {result.confidence.mean_grade}")
        logger.debug(f"   - Result type: {type(result)}")
        logger.debug(f"   - Document type: {type(result.document)}")
        logger.debug(f"   - Confidence type: {type(result.confidence)}")
        
        if hasattr(result.confidence, 'report') and result.confidence.report:
            logger.info(f"   - Number of pages in confidence report: {len(result.confidence.report)}")
            for i, page_conf in enumerate(result.confidence.report):
                logger.debug(f"     Page {i+1}: {page_conf}")
        else:
            logger.debug("   - No detailed confidence report available")
        
        # Analyze document structure
        logger.info("üîç Analyzing document structure...")
        doc = result.document
        logger.debug(f"Document attributes: {[attr for attr in dir(doc) if not attr.startswith('_')]}")
        
        if hasattr(doc, '_children') and doc._children:
            logger.info(f"   - Document has {len(doc._children)} child elements")
            for i, child in enumerate(doc._children[:5]):  # Show first 5
                logger.debug(f"     Child {i}: {type(child)} - {getattr(child, 'text', 'no text')[:50]}...")
        
        if hasattr(doc, 'pages'):
            logger.info(f"   - Document has {len(doc.pages)} pages")
            # doc.pages might be a dict, not a list
            try:
                pages_items = list(doc.pages.items())[:3] if hasattr(doc.pages, 'items') else list(doc.pages)[:3]
                for i, page_item in enumerate(pages_items):
                    if isinstance(page_item, tuple):
                        page_num, page = page_item
                        logger.debug(f"     Page {page_num}: {type(page)}")
                    else:
                        logger.debug(f"     Page {i+1}: {type(page_item)}")
            except Exception as e:
                logger.debug(f"     Could not iterate pages: {e}")
        
        logger.info("üéâ Document parsing completed successfully!")
        return result.document, result.confidence

    def serialize_document(self, doc: DoclingDocument) -> str:
        """Serialize document to markdown."""
        logger.info("üìù Starting document serialization to markdown...")
        logger.debug(f"Document type: {type(doc)}")
        
        # Create serializer
        logger.debug("üîß Creating MarkdownDocSerializer...")
        try:
            serializer = MarkdownDocSerializer(doc=doc)
            logger.debug("‚úÖ MarkdownDocSerializer created successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to create serializer: {e}")
            raise
        
        # Serialize document
        logger.info("üîÑ Serializing document...")
        try:
            ser_result = serializer.serialize()
            logger.info("‚úÖ Document serialization completed successfully")
        except Exception as e:
            logger.error(f"‚ùå Serialization failed: {e}")
            logger.error(f"Exception type: {type(e)}")
            raise
        
        # Analyze serialization result
        text_length = len(ser_result.text)
        line_count = ser_result.text.count('\n')
        word_count = len(ser_result.text.split())
        
        logger.info(f"üìä Serialization results:")
        logger.info(f"   - Total characters: {text_length:,}")
        logger.info(f"   - Total lines: {line_count:,}")
        logger.info(f"   - Total words: {word_count:,}")
        logger.debug(f"   - First 200 chars: {ser_result.text[:200]}...")
        
        if text_length == 0:
            logger.warning("‚ö†Ô∏è Serialization resulted in empty text!")
        
        logger.info("üéâ Document serialization analysis complete!")
        return ser_result.text

    def debug_chunk_provenance(self, chunk, chunk_index: int) -> dict[str, Any]:
        """Debug chunk provenance information in detail."""
        logger.info(f"üîç CHUNK {chunk_index} PROVENANCE ANALYSIS")
        logger.debug(f"=" * 60)
        
        debug_info = {
            "chunk_index": chunk_index,
            "has_prov_attr": hasattr(chunk, "prov"),
            "prov_value": None,
            "prov_type": None,
            "prov_len": None,
            "page_extraction_attempts": [],
            "chunk_type": str(type(chunk)),
            "chunk_attributes": []
        }
        
        # Analyze chunk itself
        logger.debug(f"üì¶ Chunk analysis:")
        logger.debug(f"   - Type: {type(chunk)}")
        logger.debug(f"   - Memory address: {id(chunk)}")
        
        # Get all chunk attributes
        chunk_attrs = [attr for attr in dir(chunk) if not attr.startswith('_')]
        debug_info["chunk_attributes"] = chunk_attrs
        logger.debug(f"   - Non-private attributes: {chunk_attrs}")
        
        # Special attributes to check
        special_attrs = ["text", "label", "parent", "children", "metadata"]
        for attr in special_attrs:
            if hasattr(chunk, attr):
                try:
                    value = getattr(chunk, attr)
                    logger.debug(f"   - {attr}: {type(value)} = {str(value)[:100]}...")
                except Exception as e:
                    logger.debug(f"   - {attr}: Error accessing - {e}")
        
        # Check for provenance attribute
        logger.info(f"üîó Provenance analysis:")
        if hasattr(chunk, "prov"):
            logger.info("   ‚úÖ Chunk HAS 'prov' attribute")
            debug_info["prov_value"] = str(chunk.prov)
            debug_info["prov_type"] = str(type(chunk.prov))
            
            logger.debug(f"   - Provenance value: {chunk.prov}")
            logger.debug(f"   - Provenance type: {type(chunk.prov)}")
            logger.debug(f"   - Provenance repr: {repr(chunk.prov)}")
            
            if chunk.prov is not None:
                # Get length
                try:
                    prov_len = len(chunk.prov) if hasattr(chunk.prov, '__len__') else "No __len__ method"
                    debug_info["prov_len"] = prov_len
                    logger.info(f"   - Provenance length: {prov_len}")
                except Exception as e:
                    debug_info["prov_len"] = f"Error getting length: {e}"
                    logger.debug(f"   - Error getting provenance length: {e}")
                
                # Analyze provenance structure
                logger.debug(f"   - Provenance attributes: {[attr for attr in dir(chunk.prov) if not attr.startswith('_')]}")
                
                # Try to iterate through provenance items
                try:
                    if hasattr(chunk.prov, '__iter__') and hasattr(chunk.prov, '__len__') and len(chunk.prov) > 0:
                        logger.info(f"   üìÑ Analyzing {len(chunk.prov)} provenance items:")
                        
                        for i, prov_item in enumerate(chunk.prov):
                            logger.debug(f"     Provenance item {i}:")
                            logger.debug(f"       - Type: {type(prov_item)}")
                            logger.debug(f"       - Value: {prov_item}")
                            logger.debug(f"       - Repr: {repr(prov_item)}")
                            
                            if i == 0:  # Focus on first item for page extraction
                                first_prov = prov_item
                                logger.info(f"   üéØ Analyzing first provenance item for page extraction:")
                                logger.debug(f"       - Attributes: {[attr for attr in dir(first_prov) if not attr.startswith('_')]}")
                                
                                # Try different page attribute names
                                page_attrs = ["page_no", "page_number", "page", "page_num", "page_idx", "page_id"]
                                for attr in page_attrs:
                                    attempt = {"attribute": attr, "exists": False, "value": None, "type": None}
                                    if hasattr(first_prov, attr):
                                        attempt["exists"] = True
                                        try:
                                            attr_value = getattr(first_prov, attr)
                                            attempt["value"] = attr_value
                                            attempt["type"] = str(type(attr_value))
                                            logger.info(f"       ‚úÖ Found {attr}: {attr_value} (type: {type(attr_value)})")
                                        except Exception as e:
                                            attempt["value"] = f"Error: {e}"
                                            logger.debug(f"       ‚ùå Error getting {attr}: {e}")
                                    else:
                                        logger.debug(f"       ‚ùå No {attr} attribute")
                                    debug_info["page_extraction_attempts"].append(attempt)
                                
                                # Dump ALL attributes and their values
                                logger.debug(f"   üìã ALL first provenance item attributes:")
                                for attr_name in dir(first_prov):
                                    if not attr_name.startswith('_'):
                                        try:
                                            attr_value = getattr(first_prov, attr_name)
                                            attr_type = type(attr_value)
                                            attr_str = str(attr_value)[:100] if len(str(attr_value)) > 100 else str(attr_value)
                                            logger.debug(f"       {attr_name}: {attr_type} = {attr_str}")
                                        except Exception as e:
                                            logger.debug(f"       {attr_name}: Error getting value - {e}")
                            
                            if i >= 2:  # Don't analyze too many items
                                logger.debug(f"     ... (showing only first 3 items)")
                                break
                    else:
                        logger.warning("   ‚ö†Ô∏è Provenance is not iterable or is empty")
                        
                except Exception as e:
                    logger.error(f"   ‚ùå Error analyzing provenance items: {e}")
                    
            else:
                logger.warning("   ‚ö†Ô∏è Provenance attribute exists but is None")
        else:
            logger.warning("   ‚ùå Chunk does NOT have 'prov' attribute")
            logger.debug(f"   Available attributes: {chunk_attrs}")
        
        logger.debug(f"=" * 60)
        logger.info(f"üéâ CHUNK {chunk_index} PROVENANCE ANALYSIS COMPLETE")
        
        return debug_info

    def extract_page_number(self, chunk, chunk_index: int) -> int | None:
        """Extract page number from chunk with enhanced debugging."""
        logger.info(f"üìÑ EXTRACTING PAGE NUMBER FOR CHUNK {chunk_index}")
        
        # Get detailed debug info (this will do the deep analysis)
        debug_info = self.debug_chunk_provenance(chunk, chunk_index)
        
        page_number = None
        extraction_method = None
        
        logger.info(f"üîç Attempting page number extraction...")
        
        # Method 1: Try provenance-based extraction
        if hasattr(chunk, "prov") and chunk.prov and len(chunk.prov) > 0:
            logger.info("   üìã Method 1: Provenance-based extraction")
            first_prov = chunk.prov[0]
            
            # Try standard attribute names from debug info
            for attempt in debug_info["page_extraction_attempts"]:
                if attempt["exists"] and attempt["value"] is not None:
                    # Validate the value
                    try:
                        # Try to convert to int if it's not already
                        if isinstance(attempt["value"], (int, float)):
                            page_number = int(attempt["value"])
                            extraction_method = f"provenance.{attempt['attribute']}"
                            logger.info(f"   ‚úÖ SUCCESS: Found page {page_number} via {attempt['attribute']}")
                            break
                        elif isinstance(attempt["value"], str) and attempt["value"].isdigit():
                            page_number = int(attempt["value"])
                            extraction_method = f"provenance.{attempt['attribute']}"
                            logger.info(f"   ‚úÖ SUCCESS: Found page {page_number} via {attempt['attribute']} (string->int)")
                            break
                        else:
                            logger.debug(f"   ‚ùå {attempt['attribute']} has non-numeric value: {attempt['value']}")
                    except (ValueError, TypeError) as e:
                        logger.debug(f"   ‚ùå {attempt['attribute']} conversion failed: {e}")
            
            if page_number is None:
                logger.warning("   ‚ùå No valid page number found in provenance attributes")
        else:
            logger.warning("   ‚ùå No provenance data available for extraction")
        
        # Method 2: Try alternative chunk attributes (if provenance failed)
        if page_number is None:
            logger.info("   üìã Method 2: Direct chunk attribute extraction")
            direct_page_attrs = ["page", "page_number", "page_no", "page_num"]
            for attr in direct_page_attrs:
                if hasattr(chunk, attr):
                    try:
                        value = getattr(chunk, attr)
                        if isinstance(value, (int, float)):
                            page_number = int(value)
                            extraction_method = f"chunk.{attr}"
                            logger.info(f"   ‚úÖ SUCCESS: Found page {page_number} via chunk.{attr}")
                            break
                        elif isinstance(value, str) and value.isdigit():
                            page_number = int(value)
                            extraction_method = f"chunk.{attr}"
                            logger.info(f"   ‚úÖ SUCCESS: Found page {page_number} via chunk.{attr} (string->int)")
                            break
                    except Exception as e:
                        logger.debug(f"   ‚ùå chunk.{attr} extraction failed: {e}")
            
            if page_number is None:
                logger.warning("   ‚ùå No page number found in direct chunk attributes")
        
        # Method 3: Try parent/container analysis (if still no page)
        if page_number is None:
            logger.info("   üìã Method 3: Parent/container analysis")
            if hasattr(chunk, 'parent') and chunk.parent:
                logger.debug("   - Checking parent element...")
                # Recursively check parent for page info
                # This could be extended based on document structure
            
        # Final result
        if page_number is not None:
            logger.info(f"üéâ PAGE EXTRACTION SUCCESS: Chunk {chunk_index} -> Page {page_number} (via {extraction_method})")
        else:
            logger.error(f"üí• PAGE EXTRACTION FAILED: Chunk {chunk_index} -> Page UNKNOWN")
            logger.error("   üìä Summary of failed attempts:")
            logger.error(f"   - Provenance available: {debug_info.get('has_prov_attr', False)}")
            logger.error(f"   - Provenance length: {debug_info.get('prov_len', 'N/A')}")
            logger.error(f"   - Extraction attempts: {len(debug_info.get('page_extraction_attempts', []))}")
        
        return page_number

    def chunk_document(self, doc: DoclingDocument) -> Generator[dict[str, Any]]:
        """
        Chunk document using HybridChunker while preserving page numbers.
        
        Adds detailed logging of placeholder removal and fallback extraction so we can
        inspect problematic PDFs.
        """
        serializer = MarkdownDocSerializer(doc=doc)
        placeholder_summary = {
            "chunks": 0,
            "segments": 0,
            "removed_segments": 0,
            "fallback_used": 0,
            "fallback_failed": 0,
        }

        logger.info("üß© STARTING DOCUMENT CHUNKING (USING HYBRIDCHUNKER)...")
        logger.info("=" * 80)

        chunk_index = 0
        collected_chunks: list[dict[str, Any]] = []
        for raw_chunk in self.chunker.chunk(doc):
            placeholder_summary["chunks"] += 1
            logger.debug(f"Processing chunk {chunk_index}")

            (
                combined_text,
                page_number,
                stats,
                diagnostic,
            ) = self._extract_markdown_segment(
                raw_chunk,
                serializer,
                self.chunker,
                doc,
                debug_chunk_index=chunk_index,
            )

            placeholder_summary["segments"] += stats["total_segments"]
            placeholder_summary["removed_segments"] += stats["removed_segments"]
            placeholder_summary["fallback_used"] += 1 if stats["fallback_used"] else 0
            placeholder_summary["fallback_failed"] += 1 if stats["fallback_failed"] else 0

            if not combined_text.strip():
                logger.warning(
                    "   ‚ö†Ô∏è Chunk %s produced no usable text after placeholder filtering; skipping.",
                    chunk_index,
                )
                chunk_index += 1
                continue

            segment_index = 0
            for text_segment in self._split_to_token_budget(combined_text):
                token_count = len(self._encoding.encode(text_segment))
                preview = repr(text_segment[:100] if len(text_segment) > 100 else text_segment)

                chunk_data = {
                    "chunk_id": str(uuid4()),
                    "text": text_segment,
                    "page_number": page_number,
                    "chunk_index": chunk_index,
                    "segment_index": segment_index,
                    "token_count": token_count,
                    "char_count": len(text_segment),
                    "word_count": len(text_segment.split()),
                    "diagnostic": diagnostic if self.debug_mode else None,
                    "extraction_method": "docling",
                }

                if page_number is not None:
                    logger.info("‚úÖ Chunk %s.%s (page %s) tokens=%s preview=%s",
                                chunk_index, segment_index, page_number, token_count, preview)
                else:
                    logger.warning("‚ö†Ô∏è Chunk %s.%s has unknown page, tokens=%s preview=%s",
                                   chunk_index, segment_index, token_count, preview)

                if chunk_index < 5:
                    self._log_chunk_preview(chunk_data, chunk_index, segment_index)

                collected_chunks.append(chunk_data)
                segment_index += 1
                chunk_index += 1

        logger.info("=" * 80)
        logger.info("üéâ DOCUMENT CHUNKING COMPLETE!")
        logger.info(
            "üìä Placeholder summary: chunks=%s segments=%s removed=%s fallback_used=%s fallback_failed=%s",
            placeholder_summary["chunks"],
            placeholder_summary["segments"],
            placeholder_summary["removed_segments"],
            placeholder_summary["fallback_used"],
            placeholder_summary["fallback_failed"],
        )
        fallback_needed = (
            placeholder_summary["fallback_used"] > 0
            and placeholder_summary["fallback_failed"] == placeholder_summary["fallback_used"]
        )

        if (fallback_needed or not collected_chunks) and self._current_pdf_path is not None:
            logger.warning(
                "Docling chunking left unresolved placeholders; attempting OpenAI vision fallback."
            )
            placeholder_pages = self._placeholder_pages_from_chunks(collected_chunks)
            fallback_chunks = self._chunk_with_openai_vision(placeholder_pages)
            if fallback_chunks:
                for chunk in fallback_chunks:
                    yield chunk
                return
            logger.warning("OpenAI vision fallback produced no usable chunks; using original output.")

        for chunk in collected_chunks:
            yield chunk

    def _extract_markdown_segment(
        self,
        chunk: Any,
        serializer: MarkdownDocSerializer,
        chunker: HybridChunker,
        doc: Optional[DoclingDocument],
        debug_chunk_index: int,
    ) -> tuple[str, Optional[int], dict[str, int | bool], dict[str, Any]]:
        text_parts: list[str] = []
        page_candidates: list[int] = []
        placeholder_removed = 0
        inspected = 0

        diagnostic: dict[str, Any] = {
            "chunk_index": debug_chunk_index,
            "doc_items": [],
            "fallback_candidates": [],
        }

        doc_items = getattr(getattr(chunk, "meta", None), "doc_items", None) or []

        for idx, item in enumerate(doc_items):
            inspected += 1
            resolved_item = self._resolve_doc_item(doc, item)
            resolved_text = self._serialize_doc_item(serializer, resolved_item)
            raw_text = self._serialize_doc_item(serializer, item)
            diagnostic_entry = {
                "item_index": idx,
                "resolved_len": len(resolved_text),
                "raw_len": len(raw_text),
                "resolved_preview": resolved_text[:120],
                "raw_preview": raw_text[:120],
                "resolved_placeholder": self._is_placeholder_text(resolved_text),
                "raw_placeholder": self._is_placeholder_text(raw_text),
            }

            if self._is_placeholder_text(resolved_text):
                placeholder_removed += 1
            else:
                text_parts.append(resolved_text.strip())

            page = self._first_page_number(resolved_item) or self._first_page_number(item)
            if page is not None:
                page_candidates.append(page)

            if idx < self.MAX_DEBUG_ITEMS:
                diagnostic["doc_items"].append(diagnostic_entry)

        if not text_parts:
            try:
                contextualized = chunker.contextualize(chunk)
            except Exception as exc:
                logger.warning("Failed to contextualize chunk %s: %s", debug_chunk_index, exc)
                contextualized = ""

            diagnostic["contextualized_preview"] = contextualized[:200]
            if contextualized and not self._is_placeholder_text(contextualized):
                text_parts.append(contextualized.strip())
                page = (
                    self._first_page_number(chunk.meta.doc_items[0])
                    if getattr(chunk, "meta", None) and getattr(chunk.meta, "doc_items", None)
                    else None
                )
                if page is not None:
                    page_candidates.append(page)
            else:
                if contextualized:
                    placeholder_removed += 1

        combined_text = "\n\n".join(part for part in text_parts if part)
        fallback_used = False
        fallback_failed = False

        if self._contains_gid(combined_text) or not combined_text:
            fallback_used = True
            fallback_text, candidates = self._attempt_fallback(chunk, chunker, page_candidates)
            diagnostic["fallback_candidates"] = candidates
            if fallback_text and not self._contains_gid(fallback_text):
                combined_text = fallback_text
            else:
                fallback_failed = True

        page_number = page_candidates[0] if page_candidates else None
        stats = {
            "total_segments": inspected,
            "removed_segments": placeholder_removed,
            "fallback_used": fallback_used,
            "fallback_failed": fallback_failed,
        }
        diagnostic["page_number"] = page_number
        diagnostic["combined_text_preview"] = combined_text[:200]

        return combined_text, page_number, stats, diagnostic

    def _attempt_fallback(
        self,
        chunk: Any,
        chunker: HybridChunker,
        page_candidates: list[int],
    ) -> tuple[str, list[dict[str, Any]]]:

        candidates_info: list[dict[str, Any]] = []
        candidate_texts: list[str] = []

        serialize_fn = getattr(chunker, "serialize", None)
        if callable(serialize_fn):
            try:
                serialized = serialize_fn(chunk)
                if isinstance(serialized, str) and serialized.strip():
                    candidate_texts.append(serialized.strip())
                    candidates_info.append(
                        {"source": "chunker.serialize", "len": len(serialized.strip()), "preview": serialized[:120]}
                    )
            except Exception as exc:
                logger.debug("Fallback serialize() failed: %s", exc)
                candidates_info.append({"source": "chunker.serialize", "error": str(exc)})

        try:
            contextualized = chunker.contextualize(chunk)
            if isinstance(contextualized, str) and contextualized.strip():
                candidate_texts.append(contextualized.strip())
                candidates_info.append(
                    {"source": "chunker.contextualize", "len": len(contextualized.strip()), "preview": contextualized[:120]}
                )
        except Exception as exc:
            logger.debug("Fallback contextualize() failed: %s", exc)
            candidates_info.append({"source": "chunker.contextualize", "error": str(exc)})

        page_no = page_candidates[0] if page_candidates else None
        fitz_text = self._extract_text_with_fitz(page_no)
        if fitz_text:
            candidate_texts.append(fitz_text)
            candidates_info.append(
                {"source": "pymupdf", "len": len(fitz_text), "preview": fitz_text[:120], "page": page_no}
            )

        for candidate in candidate_texts:
            if candidate and not self._contains_gid(candidate) and not self._is_placeholder_text(candidate):
                return candidate, candidates_info
        return "", candidates_info

    @staticmethod
    def _is_placeholder_text(value: str) -> bool:
        if not value:
            return True
        normalized = value.strip()
        if not normalized:
            return True
        lowered = normalized.lower()
        if lowered in StandaloneDocumentParser._PLACEHOLDER_MARKERS:
            return True
        if StandaloneDocumentParser._contains_gid(lowered):
            return True
        stripped = lowered.lstrip(" -*‚Ä¢\u2022\t")
        if StandaloneDocumentParser._contains_gid(stripped):
            return True
        return False

    @staticmethod
    def _contains_gid(value: str) -> bool:
        return StandaloneDocumentParser._GID_TOKEN in value.lower()

    @staticmethod
    def _placeholder_pages_from_chunks(
        chunks: Sequence[dict[str, Any]]
    ) -> set[int]:
        pages: set[int] = set()
        for chunk in chunks:
            page_number = chunk.get("page_number")
            if page_number is None:
                continue
            text = chunk.get("text", "")
            if StandaloneDocumentParser._is_placeholder_text(text):
                pages.add(int(page_number))
        return pages

    def _chunk_with_openai_vision(
        self,
        placeholder_pages: Optional[set[int]] = None,
    ) -> list[dict[str, Any]]:
        pdf_path = self._current_pdf_path
        if pdf_path is None:
            logger.error("Cannot run vision fallback without original PDF path.")
            return []

        if not self.vision_config.enabled:
            logger.info("Vision fallback disabled via configuration.")
            return []

        all_pages = self._list_pdf_pages(pdf_path)
        if not all_pages:
            logger.error("Unable to enumerate PDF pages for vision fallback.")
            return []

        fallback_candidates = (
            sorted(set(placeholder_pages or set()).intersection(all_pages))
            if placeholder_pages
            else list(all_pages)
        )
        if not fallback_candidates:
            fallback_candidates = list(all_pages)

        recovered = recover_pages_with_openai(pdf_path, fallback_candidates, self.vision_config)

        missing_pages = set(fallback_candidates) - set(recovered.keys())
        if missing_pages:
            logger.warning(
                "Vision fallback returned no text for pages: %s",
                sorted(missing_pages),
            )

        if not recovered:
            logger.warning("Vision fallback did not recover any text.")
            return []

        report_pages: list[PageExtraction] = []
        usable_pages: list[PageExtraction] = []

        for page_number in fallback_candidates:
            recovered_text = (recovered.get(page_number) or "").strip()
            method = f"vision:{self.vision_config.model}"
            if not recovered_text:
                report_pages.append(
                    PageExtraction(
                        page_number=page_number,
                        text="",
                        method=method,
                        needs_vision=True,
                    )
                )
                continue

            words = recovered_text.replace("\n", " ").split()
            snippet = " ".join(words[:200])
            logger.info(
                "[Vision] Page %s recovered text (first %s words): %s",
                page_number,
                min(200, len(words)),
                snippet,
            )

            page_entry = PageExtraction(
                page_number=page_number,
                text=recovered_text,
                method=method,
                needs_vision=False,
            )
            report_pages.append(page_entry)
            usable_pages.append(page_entry)

        chunks: list[dict[str, Any]] = []
        chunk_index = 0
        for page in usable_pages:
            text = page.text.strip()
            if not text:
                logger.warning("Page %s still has no extractable text; skipping.", page.page_number)
                continue

            for segment_index, text_segment in enumerate(self._split_to_token_budget(text), start=1):
                text_segment = text_segment.strip()
                if not text_segment:
                    continue

                chunk_index += 1
                token_count = len(self._encoding.encode(text_segment))
                chunk_data = {
                    "chunk_id": str(uuid4()),
                    "text": text_segment,
                    "page_number": page.page_number,
                    "chunk_index": chunk_index,
                    "segment_index": segment_index - 1,
                    "token_count": token_count,
                    "char_count": len(text_segment),
                    "word_count": len(text_segment.split()),
                    "extraction_method": page.method,
                }
                if chunk_index <= 5:
                    logger.debug("Fallback chunk %s preview: %s", chunk_index, repr(text_segment[:100]))
                chunks.append(chunk_data)

        if not chunks:
            logger.error("OpenAI vision fallback yielded no chunks.")
            return []

        self._write_fallback_report(pdf_path, report_pages, chunks)
        logger.info("OpenAI vision fallback produced %s chunks.", len(chunks))
        return chunks

    def _list_pdf_pages(self, pdf_path: Path) -> list[int]:
        if fitz is None:
            logger.error("PyMuPDF is required to enumerate PDF pages for vision fallback.")
            return []

        try:
            document = fitz.open(str(pdf_path))  # type: ignore[operator]
        except Exception as exc:
            logger.exception("Unable to open PDF with PyMuPDF: %s", exc)
            return []

        try:
            return list(range(1, document.page_count + 1))
        finally:
            document.close()

    def _write_fallback_report(
        self,
        pdf_path: Path,
        pages: list[PageExtraction],
        chunks: list[dict[str, Any]],
    ) -> None:
        reports_dir = Path(__file__).parent / "results"
        reports_dir.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_path = reports_dir / f"fallback_chunks_{timestamp}.md"

        lines: list[str] = []
        lines.append(f"# OpenAI Vision Fallback Report for {pdf_path.name}")
        lines.append("")
        lines.append(f"Generated: {datetime.now().isoformat(timespec='seconds')}")
        lines.append(f"Total fallback chunks: {len(chunks)}")
        lines.append("")

        for page in pages:
            lines.append(f"## Page {page.page_number} (method={page.method})")
            lines.append("")
            snippet = page.text.strip()
            if len(snippet) > 2000:
                snippet = snippet[:2000] + " ‚Ä¶"
            lines.append("```text")
            lines.append(snippet)
            lines.append("```")
            lines.append("")

        try:
            report_path.write_text("\n".join(lines), encoding="utf-8")
            logger.info("Fallback markdown report written to %s", report_path)
        except Exception as exc:
            logger.warning("Failed to write fallback report: %s", exc)

    @staticmethod
    def _serialize_doc_item(
        serializer: MarkdownDocSerializer, item: Optional[Any]
    ) -> str:
        if item is None:
            return ""
        try:
            result = serializer.serialize(item=item)
            return (result.text or "").strip()
        except Exception as exc:
            logger.debug("Doc item serialization failed: %s", exc)
            return (
                getattr(item, "text", "")
                or getattr(item, "orig", "")
                or getattr(item, "markdown", "")
            )

    @staticmethod
    def _first_page_number(item: Any) -> Optional[int]:
        if hasattr(item, "prov") and item.prov:
            for provenance in item.prov:
                page = getattr(provenance, "page_no", None)
                if page is not None:
                    return page
        return None

    @staticmethod
    def _resolve_doc_item(
        doc: Optional[DoclingDocument], item: Optional[Any]
    ) -> Optional[Any]:
        if item is None or doc is None:
            return item

        resolver = getattr(item, "resolve", None)
        if callable(resolver):
            try:
                resolved = resolver(doc)
                if resolved is not None:
                    return resolved
            except Exception as exc:
                logger.debug("Doc item resolve() failed: %s", exc)

        reference = getattr(item, "self_ref", None) or getattr(item, "cref", None)
        if isinstance(reference, str):
            resolved_ref = StandaloneDocumentParser._resolve_doc_reference(doc, reference)
            if resolved_ref is not None:
                return resolved_ref

        return item

    @staticmethod
    def _resolve_doc_reference(doc: DoclingDocument, reference: str) -> Optional[Any]:
        if not reference or not reference.startswith("#/"):
            return None

        target: Any = doc
        for component in reference.lstrip("#/").split("/"):
            if target is None:
                return None
            if isinstance(target, list):
                if not component.isdigit():
                    return None
                index = int(component)
                if index < 0 or index >= len(target):
                    return None
                target = target[index]
                continue
            if isinstance(target, dict):
                target = target.get(component)
                continue
            if hasattr(target, component):
                target = getattr(target, component)
                continue
            return None

        return target

    def _initialize_converter_without_ocr(self) -> None:
        cache_path = self._model_cache_path
        logger.info("üîÑ Reinitializing DocumentConverter without OCR...")
        pipeline_options = PdfPipelineOptions(
            artifacts_path=cache_path,
            do_ocr=False,
            do_table_structure=True,
        )
        self.doc_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options
                )
            }
        )
        logger.info("‚úÖ DocumentConverter ready (OCR disabled)")

    def _load_fitz_document(self, file_path: Path):
        try:
            import fitz  # type: ignore
        except ImportError:
            logger.info("PyMuPDF not available; skipping PDF text fallback.")
            return None

        try:
            return fitz.open(file_path)
        except Exception as exc:
            logger.warning("Failed to open PDF with PyMuPDF: %s", exc)
            return None

    def _extract_text_with_fitz(self, page_number: Optional[int]) -> str:
        if self._fitz_doc is None or page_number is None:
            return ""
        try:
            page_index = max(0, page_number - 1)
            if page_index >= self._fitz_doc.page_count:
                return ""
            page = self._fitz_doc.load_page(page_index)
            text = page.get_text("text")
            logger.debug(
                "PyMuPDF fallback extracted %s chars from page %s",
                len(text),
                page_number,
            )
            return text.strip()
        except Exception as exc:
            logger.debug("PyMuPDF extraction failed for page %s: %s", page_number, exc)
            return ""

    
    def _resolve_item_reference(self, doc: DoclingDocument, ref: str) -> Any:
        """
        Resolve a JSON reference like '#/texts/0' to the actual document item.
        """
        try:
            # Reference format is "#/collection_name/index"
            if ref.startswith('#/'):
                parts = ref.lstrip('#/').split('/')
                if len(parts) == 2:
                    collection_name, index_str = parts
                    index = int(index_str)
                    
                    logger.debug(f"      Resolving: {collection_name}[{index}]")
                    
                    # Get the collection from the document
                    if hasattr(doc, collection_name):
                        collection = getattr(doc, collection_name)
                        if collection and isinstance(collection, list) and index < len(collection):
                            logger.debug(f"      ‚úÖ Found item in {collection_name}[{index}]")
                            return collection[index]
                        else:
                            logger.debug(f"      ‚ùå Index {index} out of range for {collection_name}")
                    else:
                        logger.debug(f"      ‚ùå Collection {collection_name} not found in document")
        except Exception as e:
            logger.debug(f"      ‚ùå Error resolving reference {ref}: {e}")
        
        return None

    def _log_chunk_preview(self, chunk_data: dict[str, Any], chunk_index: int, segment_index: int) -> None:
        """Log detailed preview of a chunk."""
        logger.info("üìã CHUNK PREVIEW")
        logger.info("-" * 60)
        logger.info(f"üî¢ Chunk ID: {chunk_data['chunk_id']}")
        logger.info(f"üìç Position: Chunk {chunk_index}, Segment {segment_index}")
        logger.info(f"üìÑ Page Number: {chunk_data['page_number'] if chunk_data['page_number'] else '‚ùå UNKNOWN'}")
        logger.info(f"üî§ Token Count: {chunk_data['token_count']}")
        logger.info(f"üìù Character Count: {chunk_data['char_count']}")
        logger.info(f"üìñ Word Count: {chunk_data['word_count']}")
        
        # Text preview
        text = chunk_data['text']
        preview_length = 200
        if len(text) > preview_length:
            preview = text[:preview_length] + "..."
            logger.info(f"üìñ Text Preview ({preview_length} chars): {repr(preview)}")
            logger.info(f"üìè Full Text Length: {len(text)} characters")
        else:
            logger.info(f"üìñ Full Text: {repr(text)}")
        
        # Show first and last few words
        words = text.split()
        if len(words) > 10:
            first_words = " ".join(words[:5])
            last_words = " ".join(words[-5:])
            logger.info(f"üî§ First 5 words: {repr(first_words)}")
            logger.info(f"üî§ Last 5 words: {repr(last_words)}")
        
        # Debug info summary
        if chunk_data.get('debug_info'):
            debug = chunk_data['debug_info']
            logger.info(f"üîç Debug Info:")
            logger.info(f"   - Has provenance: {debug.get('has_prov_attr', False)}")
            logger.info(f"   - Provenance length: {debug.get('prov_len', 'N/A')}")
            logger.info(f"   - Page extraction attempts: {len(debug.get('page_extraction_attempts', []))}")
            
            # Show successful page extraction method if any
            for attempt in debug.get('page_extraction_attempts', []):
                if attempt.get('exists') and attempt.get('value') is not None:
                    logger.info(f"   ‚úÖ Page found via: {attempt['attribute']} = {attempt['value']}")
                    break
            else:
                logger.info(f"   ‚ùå No page extraction succeeded")
        
        logger.info("-" * 60)

    def _log_chunking_summary(self, all_chunks: list[dict[str, Any]]) -> None:
        """Log summary statistics about all chunks."""
        total_chunks = len(all_chunks)
        chunks_with_pages = [c for c in all_chunks if c['page_number'] is not None]
        chunks_without_pages = [c for c in all_chunks if c['page_number'] is None]
        
        logger.info("üìä CHUNKING SUMMARY")
        logger.info("=" * 60)
        logger.info(f"üìä Total chunks generated: {total_chunks}")
        logger.info(f"‚úÖ Chunks with page numbers: {len(chunks_with_pages)} ({len(chunks_with_pages)/total_chunks*100:.1f}%)")
        logger.info(f"‚ùå Chunks without page numbers: {len(chunks_without_pages)} ({len(chunks_without_pages)/total_chunks*100:.1f}%)")
        
        if chunks_with_pages:
            page_numbers = [c['page_number'] for c in chunks_with_pages]
            logger.info(f"üìÑ Page range: {min(page_numbers)} - {max(page_numbers)}")
            logger.info(f"üìÑ Unique pages: {len(set(page_numbers))}")
        
        # Token statistics
        token_counts = [c['token_count'] for c in all_chunks]
        chunks_below_min = [c for c in all_chunks if c['token_count'] < self.MIN_CHUNK_TOKENS]
        chunks_above_max = [c for c in all_chunks if c['token_count'] > self.MAX_CHUNK_TOKENS]
        chunks_in_range = [c for c in all_chunks if self.MIN_CHUNK_TOKENS <= c['token_count'] <= self.MAX_CHUNK_TOKENS]
        
        logger.info(f"üî¢ Token statistics:")
        logger.info(f"   - Total tokens: {sum(token_counts):,}")
        logger.info(f"   - Average tokens per chunk: {sum(token_counts)/len(token_counts):.1f}")
        logger.info(f"   - Min tokens: {min(token_counts)}")
        logger.info(f"   - Max tokens: {max(token_counts)}")
        logger.info(f"   - Target range: {self.MIN_CHUNK_TOKENS} - {self.MAX_CHUNK_TOKENS} tokens")
        logger.info(f"   ‚úÖ Chunks in target range: {len(chunks_in_range)} ({len(chunks_in_range)/total_chunks*100:.1f}%)")
        
        if chunks_below_min:
            logger.warning(f"   ‚ö†Ô∏è Chunks below minimum ({self.MIN_CHUNK_TOKENS}): {len(chunks_below_min)} ({len(chunks_below_min)/total_chunks*100:.1f}%)")
            for i, chunk in enumerate(chunks_below_min[:3]):
                logger.warning(f"     {i+1}. Chunk {chunk['chunk_index']}: {chunk['token_count']} tokens")
        
        if chunks_above_max:
            logger.warning(f"   ‚ö†Ô∏è Chunks above maximum ({self.MAX_CHUNK_TOKENS}): {len(chunks_above_max)} ({len(chunks_above_max)/total_chunks*100:.1f}%)")
            for i, chunk in enumerate(chunks_above_max[:3]):
                logger.warning(f"     {i+1}. Chunk {chunk['chunk_index']}: {chunk['token_count']} tokens")
        
        # Character statistics
        char_counts = [c['char_count'] for c in all_chunks]
        logger.info(f"üìù Character statistics:")
        logger.info(f"   - Total characters: {sum(char_counts):,}")
        logger.info(f"   - Average characters per chunk: {sum(char_counts)/len(char_counts):.1f}")
        
        # Show sample of chunks without pages for debugging
        if chunks_without_pages and len(chunks_without_pages) <= 10:
            logger.warning("‚ùå CHUNKS WITHOUT PAGE NUMBERS:")
            for i, chunk in enumerate(chunks_without_pages[:5]):
                logger.warning(f"   {i+1}. Chunk {chunk['chunk_index']}: {chunk['token_count']} tokens, text: {chunk['text'][:50]}...")
        elif len(chunks_without_pages) > 10:
            logger.warning(f"‚ùå {len(chunks_without_pages)} chunks without page numbers (showing first 3):")
            for i, chunk in enumerate(chunks_without_pages[:3]):
                logger.warning(f"   {i+1}. Chunk {chunk['chunk_index']}: {chunk['token_count']} tokens")
        
        logger.info("=" * 60)

    def _split_to_token_budget(self, text: str) -> Iterable[str]:
        """
        Split text to fit within token budget while ensuring minimum chunk size.
        
        Ensures chunks have at least MIN_CHUNK_TOKENS (1500) tokens and 
        at most MAX_CHUNK_TOKENS (2000) tokens.
        """
        tokens = self._encoding.encode(text)
        total_tokens = len(tokens)
        
        logger.debug(f"Splitting text with {total_tokens} tokens (min: {self.MIN_CHUNK_TOKENS}, max: {self.MAX_CHUNK_TOKENS})")
        
        # If text is smaller than minimum, return as single chunk
        if total_tokens <= self.MIN_CHUNK_TOKENS:
            logger.debug(f"Text has {total_tokens} tokens, below minimum {self.MIN_CHUNK_TOKENS}, returning as single chunk")
            return [text]
        
        # If text fits within maximum, return as single chunk
        if total_tokens <= self.MAX_CHUNK_TOKENS:
            logger.debug(f"Text has {total_tokens} tokens, within max {self.MAX_CHUNK_TOKENS}, returning as single chunk")
            return [text]

        segments: list[str] = []
        start = 0
        
        while start < total_tokens:
            # Try to create a chunk with maximum tokens
            end = min(start + self.MAX_CHUNK_TOKENS, total_tokens)
            segment_tokens = tokens[start:end]
            current_segment_size = len(segment_tokens)
            
            # Check if this would be the last chunk and if it's too small
            remaining_tokens = total_tokens - end
            next_chunk_would_be_too_small = (remaining_tokens > 0 and 
                                           remaining_tokens < self.MIN_CHUNK_TOKENS)
            
            if next_chunk_would_be_too_small:
                # Adjust current chunk size to ensure next chunk meets minimum
                # Calculate how many tokens we need to leave for the next chunk
                tokens_for_next = self.MIN_CHUNK_TOKENS
                available_for_current = total_tokens - start - tokens_for_next
                
                if available_for_current >= self.MIN_CHUNK_TOKENS:
                    # We can make current chunk smaller and still meet minimum
                    end = start + available_for_current
                    segment_tokens = tokens[start:end]
                    current_segment_size = len(segment_tokens)
                    logger.debug(f"Adjusted chunk size to {current_segment_size} tokens to ensure next chunk meets minimum")
                else:
                    # Current chunk would be too small if we adjust, so extend it to include remaining tokens
                    end = total_tokens
                    segment_tokens = tokens[start:end]
                    current_segment_size = len(segment_tokens)
                    logger.debug(f"Extended final chunk to {current_segment_size} tokens to avoid small remainder")
            
            segments.append(self._encoding.decode(segment_tokens))
            logger.debug(f"Created chunk with {current_segment_size} tokens (position {start}-{end})")
            
            if end == total_tokens:
                break
                
            # Calculate next start position with overlap, but ensure we don't create too small chunks
            next_start = max(end - self.CHUNK_OVERLAP_TOKENS, start + 1)
            remaining_after_next = total_tokens - next_start
            
            # If the remaining text after next start would be too small, adjust
            if remaining_after_next < self.MIN_CHUNK_TOKENS and remaining_after_next > 0:
                # Move start back to create a larger final chunk
                next_start = total_tokens - self.MIN_CHUNK_TOKENS
                logger.debug(f"Adjusted next start to {next_start} to ensure final chunk meets minimum size")
            
            start = next_start
        
        logger.info(f"Split {total_tokens} tokens into {len(segments)} segments")
        for i, segment in enumerate(segments):
            segment_token_count = len(self._encoding.encode(segment))
            logger.info(f"  Segment {i}: {segment_token_count} tokens (min: {self.MIN_CHUNK_TOKENS}, max: {self.MAX_CHUNK_TOKENS})")
            
            # Validate segment meets requirements
            if segment_token_count < self.MIN_CHUNK_TOKENS:
                logger.warning(f"  ‚ö†Ô∏è Segment {i} has only {segment_token_count} tokens, below minimum {self.MIN_CHUNK_TOKENS}")
            if segment_token_count > self.MAX_CHUNK_TOKENS:
                logger.warning(f"  ‚ö†Ô∏è Segment {i} has {segment_token_count} tokens, above maximum {self.MAX_CHUNK_TOKENS}")
        
        return segments


class LocalStorage:
    """Local storage for test results."""
    
    def __init__(self, base_path: Path):
        self.base_path = base_path
        self.base_path.mkdir(exist_ok=True)
    
    def save_parsing_result(self, file_path: Path, document: DoclingDocument, 
                          confidence: ConfidenceReport, content: str, 
                          chunks: list[dict[str, Any]]) -> str:
        """Save parsing results to local storage."""
        logger.info("üíæ STARTING RESULT SAVE PROCESS...")
        
        timestamp = datetime.now().isoformat()
        result_id = str(uuid4())
        
        # Analyze chunks for summary
        chunks_with_pages = [c for c in chunks if c.get('page_number') is not None]
        chunks_without_pages = [c for c in chunks if c.get('page_number') is None]
        
        result = {
            "result_id": result_id,
            "timestamp": timestamp,
            "file_path": str(file_path),
            "file_name": file_path.name,
            "file_size": file_path.stat().st_size,
            "confidence": {
                "mean_grade": str(confidence.mean_grade),
                "num_pages": len(confidence.report) if hasattr(confidence, 'report') and confidence.report else 0
            },
            "content_length": len(content),
            "chunk_count": len(chunks),
            "chunks_with_pages": len(chunks_with_pages),
            "chunks_without_pages": len(chunks_without_pages),
            "page_extraction_success_rate": len(chunks_with_pages) / len(chunks) * 100 if chunks else 0,
            "chunks": chunks
        }
        
        logger.info(f"üìä Result summary:")
        logger.info(f"   - Result ID: {result_id}")
        logger.info(f"   - Total chunks: {len(chunks)}")
        logger.info(f"   ‚úÖ Chunks with pages: {len(chunks_with_pages)} ({len(chunks_with_pages)/len(chunks)*100:.1f}%)")
        logger.info(f"   ‚ùå Chunks without pages: {len(chunks_without_pages)} ({len(chunks_without_pages)/len(chunks)*100:.1f}%)")
        
        # Save main result
        result_file = self.base_path / f"result_{result_id}.json"
        try:
            with open(result_file, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False, default=str)
            logger.info(f"‚úÖ SUCCESS: Results saved to {result_file}")
        except Exception as e:
            logger.error(f"‚ùå FAILED: Could not save results: {e}")
            raise
        
        # Save content separately
        content_file = self.base_path / f"content_{result_id}.md"
        try:
            with open(content_file, "w", encoding="utf-8") as f:
                f.write(content)
            logger.info(f"‚úÖ SUCCESS: Content saved to {content_file}")
        except Exception as e:
            logger.error(f"‚ùå FAILED: Could not save content: {e}")
            raise
        
        # Save a human-readable summary
        summary_file = self.base_path / f"summary_{result_id}.txt"
        try:
            with open(summary_file, "w", encoding="utf-8") as f:
                f.write(f"PARSING RESULTS SUMMARY\n")
                f.write(f"{'='*50}\n\n")
                f.write(f"File: {file_path.name}\n")
                f.write(f"Processed: {timestamp}\n")
                f.write(f"Confidence: {confidence.mean_grade}\n")
                f.write(f"Content Length: {len(content):,} characters\n")
                f.write(f"Total Chunks: {len(chunks)}\n")
                f.write(f"‚úÖ Chunks with pages: {len(chunks_with_pages)} ({len(chunks_with_pages)/len(chunks)*100:.1f}%)\n")
                f.write(f"‚ùå Chunks without pages: {len(chunks_without_pages)} ({len(chunks_without_pages)/len(chunks)*100:.1f}%)\n\n")
                
                if chunks_with_pages:
                    page_numbers = sorted(set(c['page_number'] for c in chunks_with_pages))
                    f.write(f"Page range: {min(page_numbers)} - {max(page_numbers)}\n")
                    f.write(f"Unique pages: {len(page_numbers)}\n\n")
                
                f.write("FIRST 3 CHUNKS PREVIEW:\n")
                f.write("-" * 30 + "\n")
                for i, chunk in enumerate(chunks[:3]):
                    f.write(f"Chunk {i}:\n")
                    f.write(f"  Page: {chunk.get('page_number', 'UNKNOWN')}\n")
                    f.write(f"  Tokens: {chunk.get('token_count', 0)}\n")
                    f.write(f"  Text: {chunk.get('text', '')[:100]}...\n\n")
            
            logger.info(f"‚úÖ SUCCESS: Summary saved to {summary_file}")
        except Exception as e:
            logger.error(f"‚ùå FAILED: Could not save summary: {e}")
            # Don't raise here, summary is nice-to-have
        
        logger.info("üéâ SUCCESS: All result files saved successfully!")
        return result_id


def main():
    """Main function for standalone testing."""
    parser = argparse.ArgumentParser(
        description="Standalone PDF Parser for Testing and Debugging"
    )
    parser.add_argument(
        "file_path", 
        type=Path, 
        help="Absolute path to the PDF file to parse"
    )
    parser.add_argument(
        "--no-debug", 
        action="store_true", 
        help="Disable debug mode"
    )
    parser.add_argument(
        "--output-dir", 
        type=Path, 
        default=Path(__file__).parent / "results",
        help="Output directory for results"
    )
    
    args = parser.parse_args()
    
    # Validate file path
    if not args.file_path.exists():
        logger.error(f"File does not exist: {args.file_path}")
        sys.exit(1)
    
    if not args.file_path.is_file():
        logger.error(f"Path is not a file: {args.file_path}")
        sys.exit(1)
    
    # Initialize components
    debug_mode = not args.no_debug
    doc_parser = StandaloneDocumentParser(debug_mode=debug_mode)
    storage = LocalStorage(args.output_dir)
    
    try:
        logger.info(f"Starting processing of: {args.file_path}")
        
        # Parse document
        document, confidence = doc_parser.parse_document_from_file(args.file_path)
        
        # Serialize to markdown
        content = doc_parser.serialize_document(document)
        
        # Chunk document
        chunks = list(doc_parser.chunk_document(document))
        
        # Save results
        result_id = storage.save_parsing_result(
            args.file_path, document, confidence, content, chunks
        )
        
        # Print summary
        print(f"\n{'='*60}")
        print("PARSING COMPLETE")
        print(f"{'='*60}")
        print(f"File: {args.file_path.name}")
        print(f"Result ID: {result_id}")
        print(f"Confidence: {confidence.mean_grade}")
        print(f"Content length: {len(content)} characters")
        print(f"Total chunks: {len(chunks)}")
        
        # Page number summary
        pages_found = [c["page_number"] for c in chunks if c["page_number"] is not None]
        pages_unknown = len([c for c in chunks if c["page_number"] is None])
        
        print(f"Chunks with page numbers: {len(pages_found)}")
        print(f"Chunks with unknown pages: {pages_unknown}")
        
        if pages_found:
            print(f"Page range: {min(pages_found)} - {max(pages_found)}")
        
        print(f"Results saved to: {args.output_dir}")
        print(f"{'='*60}")
        
    except Exception as e:
        logger.error(f"Error processing document: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
